{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your download is unstable, reduce the number of thread.\n",
    "You will need boto3 and tqdm to run this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple, List\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "DEFAULT_NUM_THREAD = 8\n",
    "S3_BUCKET = 'origin-ai-medical-data'\n",
    "AWS_ACCESS_KEY_ID = ''\n",
    "AWS_SECRET_ACCESS_KEY = ''\n",
    "DATETIME_FORMAT = '%Y-%m-%d %H:%M:%S.%f'\n",
    "\n",
    "\n",
    "class S3IO:\n",
    "    def __init__(self, bucket) -> None:\n",
    "        self._client = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    "        )\n",
    "\n",
    "        self._bucket = bucket\n",
    "\n",
    "    # TODO: might need a lazy loading approach (generator) if dataset is huge\n",
    "    def _get_uploaded_data(self, prefix: str) -> Tuple[List, List]:\n",
    "        keys = []\n",
    "        dirs = []\n",
    "        next_token = ''\n",
    "        base_kwargs = {\n",
    "            'Bucket': self._bucket,\n",
    "            'Prefix': prefix,\n",
    "        }\n",
    "        while next_token is not None:\n",
    "            kwargs = base_kwargs.copy()\n",
    "            if next_token != '':\n",
    "                kwargs.update({'ContinuationToken': next_token})\n",
    "            results = self._client.list_objects_v2(**kwargs)\n",
    "            contents = results.get('Contents')\n",
    "            \n",
    "            for i in contents:\n",
    "    \n",
    "                k = i.get('Key')\n",
    "                if k[-1] != '/':\n",
    "                    keys.append(k)\n",
    "                else:\n",
    "                    dirs.append(k)\n",
    "            next_token = results.get('NextContinuationToken')\n",
    "        return keys, dirs\n",
    "\n",
    "    def _check_and_download(self, key: str,\n",
    "                            local: str) -> None:\n",
    "\n",
    "        dest_pathname = os.path.join(local, key)\n",
    "        if not os.path.exists(os.path.dirname(dest_pathname)):\n",
    "            try:\n",
    "                os.makedirs(os.path.dirname(dest_pathname))\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Possible corner case: modified local file will still get ignored\n",
    "        if not os.path.exists(dest_pathname):\n",
    "            self._client.download_file(self._bucket, key, dest_pathname)\n",
    "\n",
    "    # Modified solution from\n",
    "    # https://stackoverflow.com/questions/31918960/boto3-to-download-all-files-from-a-s3-bucket\n",
    "    def download_directory(self, prefix: str,\n",
    "                           local: str) -> None:\n",
    "        keys, dirs = self._get_uploaded_data(prefix)\n",
    "\n",
    "        for d in dirs:\n",
    "            dest_pathname = os.path.join(local, d)\n",
    "            if not os.path.exists(os.path.dirname(dest_pathname)): \n",
    "                os.makedirs(os.path.dirname(dest_pathname))\n",
    "\n",
    "        def download_function(key: str, local_dir: str = local) -> None:\n",
    "            self._check_and_download(key, local_dir)  # type: ignore\n",
    "\n",
    "        with ThreadPoolExecutor(\n",
    "                max_workers=DEFAULT_NUM_THREAD) as executor:\n",
    "            executor.map(download_function, keys)\n",
    "\n",
    "    def _upload_file(self, file: str,\n",
    "                     prefix: str) -> None:\n",
    "        num_dir_to_skip = len(prefix.split('/')[:-1])\n",
    "\n",
    "        target_dir = '/'.join(file.split('/')[num_dir_to_skip:])\n",
    "\n",
    "        self._client.upload_file(file, self._bucket,\n",
    "                                 target_dir)\n",
    "\n",
    "    def upload_directory(self, local_prefix: str, cloud_prefix: str,\n",
    "                         num_local_dir_to_skip: int) -> None:\n",
    "        local_files = [str(file) for file\n",
    "                       in pathlib.Path(local_prefix + '/').rglob('*')\n",
    "                       if file.is_file()]\n",
    "    # while counting the num_local_dir_to_skip count the num of backslash in the local path    \n",
    "\n",
    "        def upload_function(file: str,\n",
    "                            prefix_dir: str = cloud_prefix\n",
    "                            ) -> None:\n",
    "            local_dir = '/'.join(file.split('/')[num_local_dir_to_skip:])\n",
    "            self.upload_file(file, prefix_dir + local_dir)\n",
    "\n",
    "        with ThreadPoolExecutor(\n",
    "                max_workers=DEFAULT_NUM_THREAD) as executor:\n",
    "            list(tqdm(executor.map(\n",
    "                upload_function, local_files),\n",
    "                total=len(local_files), leave=False))\n",
    "\n",
    "    def upload_file(self, local_file_path: str, cloud_file_path: str) -> None:\n",
    "        self._client.upload_file(local_file_path, self._bucket,\n",
    "                                 cloud_file_path)\n",
    "        \n",
    "    def download_from_list(file_list, local_target, max_workers=DEFAULT_NUM_THREAD):\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            list(tqdm(executor.map(\n",
    "                s3.download_directory, file_list,\n",
    "                [local_target] * len(file_list)),\n",
    "                total=len(file_list)))          \n",
    "\n",
    "s3 = S3IO(S3_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "49209\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"\")\n",
    "# raw_files = df['image_path'].values.tolist()\n",
    "# print(type(raw_files))\n",
    "# print(len(raw_files))\n",
    "\n",
    "f = open('data.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "file_list = []\n",
    "for obj in data['data']:\n",
    "    file_list.append(obj['image_path'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tranche_5/extracted_resources/patient_788e7063d0224b9589ac2b1bf12f329f/2fe7984baed04dc891717ee45c91d82d_00470.png'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is the target that you want to download to\n",
    "download_target = '/home/ubuntu/OH/OH-Classifier-Framework-Basics/first_trimester_data/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_list(file_list, local_target, max_workers=DEFAULT_NUM_THREAD):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        list(tqdm(executor.map(\n",
    "            s3.download_directory, file_list,\n",
    "            [local_target] * len(file_list)),\n",
    "            total=len(file_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This starts the download in the specified folder(this can take some time)\n",
    "# download_from_list(raw_files, download_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49209/49209 [00:00<00:00, 229494.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# import tqdm\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# df = pd.read_csv(\"/home/ubuntu/MODEL_CATALOG/Download_from_s3/csvs/model_catalog_train_test_val_data.csv\")\n",
    "# df.head()\n",
    "# for i in tqdm(df['image_path']):\n",
    "#     if os.path.exists(f\"/home/ubuntu/MODEL_CATALOG/OH-classifier-framework/data/{i}\"):\n",
    "#         #print(\"true\")\n",
    "#         pass\n",
    "#     else:\n",
    "#         print(\"path doest not exist\")\n",
    "#         print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = S3IO(S3_BUCKET)\n",
    "s3_path = 'ai-dev/.../dataset_version_2/'\n",
    "local_path = \"/.../dataset_folders/\"\n",
    "s3_client.download_directory(s3_path, local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ed860681db46afb1331880200cb335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3_client = S3IO(S3_BUCKET)\n",
    "local_file_path = \"/hom..../ataset_version_2\"\n",
    "cloud_file_path = \"ai-dev.../\"\n",
    "\n",
    "s3_client.upload_directory(local_file_path, cloud_file_path,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import string\n",
    "import shutil\n",
    "\n",
    "#Generate the file paths to traverse, or a single path if a file name was given\n",
    "def getfiles(path):\n",
    "    if os.path.isdir(path):\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for name in files:\n",
    "                yield os.path.join(root, name)\n",
    "    else:\n",
    "        yield path\n",
    "\n",
    "destination = \"./anomalyimages/\"\n",
    "fromdir = \".//\"\n",
    "for f in getfiles(fromdir):\n",
    "    filename = f.split( '/')[-1]\n",
    "    # if os.path.isfile(destination+filename):\n",
    "    filename = f.replace(fromdir,\"\",1).replace(\"/\",\"_\")\n",
    "    #os.rename(f, destination+filename)\n",
    "    shutil.copy(f, destination+filename)\n",
    "\n",
    "destination = \"./anomalyimages/\"\n",
    "fromdir = \"./anomalytctv/anomaly_curation/\"\n",
    "for f in getfiles(fromdir):\n",
    "    filename = f.split( '/')[-1]\n",
    "    # if os.path.isfile(destination+filename):\n",
    "    filename = f.replace(fromdir,\"\",1).replace(\"/\",\"_\")\n",
    "    #os.rename(f, destination+filename)\n",
    "    shutil.copy(f, destination+filename)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "822ce188d9bce5372c4adbb11364eeb49293228c2224eb55307f4664778e7f56"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
